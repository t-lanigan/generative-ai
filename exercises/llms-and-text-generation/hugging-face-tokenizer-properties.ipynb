{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Properties of Hugging Face's Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of great features when using tokenizers in Hugging Face that can make it very simple to try out and use different models. Here we'll breifly discuss some properties that can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load a couple different models:\n",
    "\n",
    "* `bert-base-cased` ([doc](https://huggingface.co/docs/transformers/model_doc/bert))\n",
    "* `xlm-roberta-base` ([doc](https://huggingface.co/docs/transformers/model_doc/xlm-roberta))\n",
    "* `google/pegasus-xsum` ([doc](https://huggingface.co/docs/transformers/model_doc/pegasus))\n",
    "* `allenai/longformer-base-4096` ([doc](https://huggingface.co/docs/transformers/model_doc/longformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 25.0/25.0 [00:00<00:00, 118kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_names = (\n",
    "    'bert-base-cased',\n",
    "    'xlm-roberta-base',\n",
    "    'google/pegasus-xsum',\n",
    "    'allenai/longformer-base-4096',\n",
    ")\n",
    "\n",
    "model_tokenizers = {\n",
    "    model_name: AutoTokenizer.from_pretrained(model_name)\n",
    "    for model_name in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `model_max_length`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models that tokenizers are associated with can only take in a maximum number of tokens and so the tokenizer might not be equipped to encode a very long sequence. It might not always be relevant, but you can find this length with `.model_max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tmax length: 4096\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    max_length = temp_tokenizer.model_max_length\n",
    "    print(f'{model_name}\\n\\tmax length: {max_length}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already mentioned special tokens like the \"unknown\" token. Different models use different ways to distinguish special tokens and not all models cover all the special tokens since it's dependent on the model's task it was trained for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tspecial tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tspecial tokens: ['</s>', '<unk>', '<pad>', '<mask_2>', '<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    special_tokens = temp_tokenizer.all_special_tokens\n",
    "    print(f'{model_name}\\n\\tspecial tokens: {special_tokens}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yout can also call the specific token you're interested in to see its representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizers['bert-base-cased'].unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='[UNK]'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token=None\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token=None\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='[MASK]'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='[SEP]'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='[CLS]'\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token='<s>'\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='</s>'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='<s>'\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token=None\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask_2>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token=None\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token=None\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token='<s>'\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='</s>'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='<s>'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    print(f'{model_name}')\n",
    "    print(f'\\tUnknown: \\n\\t\\t{temp_tokenizer.unk_token=}')\n",
    "    print(f'\\tBeginning of Sequence: \\n\\t\\t{temp_tokenizer.bos_token=}')\n",
    "    print(f'\\tEnd of Sequence: \\n\\t\\t{temp_tokenizer.eos_token=}')\n",
    "    print(f'\\tMask: \\n\\t\\t{temp_tokenizer.mask_token=}')\n",
    "    print(f'\\tSentence Separator: \\n\\t\\t{temp_tokenizer.sep_token=}')\n",
    "    print(f'\\tClass of Input: \\n\\t\\t{temp_tokenizer.cls_token=}')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
