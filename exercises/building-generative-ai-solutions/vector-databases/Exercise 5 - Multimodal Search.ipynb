{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c17377",
   "metadata": {},
   "source": [
    "# Multimodal Search\n",
    "\n",
    "In this final exercise, we will learn how to use vector databases to search through images using natural language. \n",
    "\n",
    "We will be searching through an open source image dataset using an open source model called CLIP.\n",
    "This model is able to encode both images and text into the same embedding space, allowing us to retrieve images that are similar to a user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab7501c-ae21-462f-b744-c9fc3f6ad965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --quiet datasets gradio lancedb pandas transformers [This has been preinstalled for you]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfec2b3",
   "metadata": {},
   "source": [
    "## Setup CLIP model\n",
    "\n",
    "First, let's prepare the [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) model to encode the images.\n",
    "We want to setup two things:\n",
    "1. a model to encode the image\n",
    "2. a processor to prepare the image to be encoded\n",
    "\n",
    "Fill in the code below to initialize a pre-trained model and processor.\n",
    "\n",
    "**SOLUTION** By reading the CLIP documentation, we see that we can use the `from_pretrained` classmethods to initialize the model and the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d25d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784f1db",
   "metadata": {},
   "source": [
    "## Setup data model\n",
    "\n",
    "The dataset itself has an image field and an integer label.\n",
    "We'll also need an embedding vector (CLIP produces 512D vectors) field.\n",
    "\n",
    "For this problem, please a field named \"vector\" to the Image class below\n",
    "that is a 512D vector.\n",
    "\n",
    "The image that comes out of the raw dataset is a PIL image. So we'll add\n",
    "some conversion code between PIL and bytes to make it easier for serde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027bd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from lancedb.pydantic import LanceModel, vector\n",
    "import PIL\n",
    "\n",
    "class Image(LanceModel):\n",
    "    image: bytes\n",
    "    label: int\n",
    "    vector: vector(512)\n",
    "        \n",
    "    def to_pil(self):\n",
    "        return PIL.Image.open(io.BytesIO(self.image))\n",
    "    \n",
    "    @classmethod\n",
    "    def pil_to_bytes(cls, img) -> bytes:\n",
    "        buf = io.BytesIO()\n",
    "        img.save(buf, format=\"PNG\")\n",
    "        return buf.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44277d19",
   "metadata": {},
   "source": [
    "## Image processing function\n",
    "\n",
    "Next we will implement a function to process batches of data from the dataset.\n",
    "We will be using the `zh-plus/tiny-imagenet` dataset from huggingface datasets.\n",
    "This dataset has an `image` and a `label` column.\n",
    "\n",
    "For this problem, please fill in the code to extract the image embeddings from\n",
    "the image using the CLIP model.\n",
    "\n",
    "**SOLUTION** Here we'll use the `get_image_features` method on the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c040600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(batch: dict) -> dict:\n",
    "    image = processor(text=None, images=batch[\"image\"], return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].to(device)\n",
    "    img_emb = model.get_image_features(image)\n",
    "    batch[\"vector\"] = img_emb.cpu()\n",
    "    batch[\"image_bytes\"] = [Image.pil_to_bytes(img) for img in batch[\"image\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ed9f2",
   "metadata": {},
   "source": [
    "## Table creation\n",
    "\n",
    "Please create a LanceDB table called `image_search` to store the image, label, and vector.\n",
    "\n",
    "**SOLUTION** this should be familiar to you by now. Let's call `lancedb.connect` then\n",
    "`db.create_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64a6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"~/.lancedb\")\n",
    "TABLE_NAME = \"image_search\"\n",
    "db.drop_table(TABLE_NAME, ignore_missing=True)\n",
    "table = db.create_table(TABLE_NAME, schema=Image.to_arrow_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0cc14",
   "metadata": {},
   "source": [
    "## Adding data\n",
    "\n",
    "Now we're ready to process the images and generate embeddings.\n",
    "Please write a function called `datagen` that calls `process_image` on each image in the validation set (10K images) and return a list of Image instances.\n",
    "\n",
    "**HINT**\n",
    "1. You may find it faster to use the [dataset.map](https://huggingface.co/docs/datasets/process#map) function.\n",
    "2. You'll want to store the `image_bytes` field that is returned by `process_image`.\n",
    "\n",
    "**SOLUTION**\n",
    "1. We'll call `load_dataset` and retrieve the `valid` split\n",
    "2. We then map `process_image` and construct Image instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f40c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def datagen() -> list[Image]:\n",
    "    dataset = load_dataset(\"zh-plus/tiny-imagenet\")['valid']\n",
    "    return [Image(image=b[\"image_bytes\"],\n",
    "                  label=b[\"label\"],\n",
    "                  vector=b[\"vector\"])\n",
    "            for b in dataset.map(process_image, batched=True, batch_size=32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fc633",
   "metadata": {},
   "source": [
    "Now call the function you just wrote and add the generated instances to the LanceDB table. The following process can take up to 60 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a5c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = datagen()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.add(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb70b75",
   "metadata": {},
   "source": [
    "## Encoding user queries\n",
    "\n",
    "We have image embeddings, but how do we generate the embeddings for the user query?\n",
    "Furthermore, how can we possibly have the same features between the image embeddings\n",
    "and text embeddings. This is where the power of CLIP comes in.\n",
    "\n",
    "Please write a function to turn user query text into an embedding\n",
    "in the same latent space as the images. \n",
    "\n",
    "**HINT** \n",
    "You can refer to the [CLIPModel documention](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460a8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizerFast\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(MODEL_ID)\n",
    "\n",
    "def embed_func(query):\n",
    "    inputs = tokenizer([query], padding=True, return_tensors=\"pt\")\n",
    "    text_features = model.get_text_features(**inputs)\n",
    "    return text_features.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487085e",
   "metadata": {},
   "source": [
    "## Core search function\n",
    "\n",
    "Now let's write the core search function `find_images`, that takes a text query as input, and returns a list of PIL images that's most similar to the query.\n",
    "\n",
    "**SOLUTION**\n",
    "First, we need to call `embed_func` on the query to generate the text embedding.\n",
    "Then, we'll search through the LanceDB to get images most similar to the query.\n",
    "And we'll convert the resrults into Image instances.\n",
    "Finally, we'll call the `Image.to_pil` method to convert the bytes into PIL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c712bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_images(query):\n",
    "    emb = embed_func(query)\n",
    "    rs = table.search(emb).limit(9).to_pydantic(Image)\n",
    "    return [m.to_pil() for m in rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ad4f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDjkBBqyg9qc/llx5abVAA69T605Qc4rm5meM7MUAUpQN0q7HY3TrlYmPsRg017WaM5khdPcijnIcWkU/KXuKYUI+7Vor7U1hT5iLsqOAWLEdaZKrModhx0BPerEgGKgk5PCHHvRcuLL1tarLFKP41wQPbv/SrmnRxxEu2N3bParEFiImDrnNUkgleTaOuefasb3HTmmzprOdD3BrUhljJ5VSveuU+W0QBJC0mfmHanJfyDI5IPap5UzfmsbWqaFb3EZmtQI5OuwdG/CuQkXYxVsgg4IIrpdLuREC5VVDHjjAH19Kr6lbwXd00qBAxHz7DwT604trRmNZK3Mc8EU9x9M1GAobJIxWo+nYzhiM1D9gZD91SPU1pdGHMjpRGrAYxVae1bnyk2sxyWOMU/IhxuyaYdQUnCg59MViTC8diobGVSRtz+NK8BghaR1CgDvyTV2GSV1AKH3J4zUt1ayXNo0aqoY4wSeOtFzRTl1Mi0T7QSWTIUdia1o4UVQFHuansrFbSBUOC3Vj6mrJWLIywGfwobIm3JlE24PJUgVA0ShsE1qGMSL8rhl/PNUpIVWQKT8zZIoTI5RuozrGGRdvTkisEXgjnJHPqBWpeo5c/KMHnINZBt2I+7+Iojax0KJr2uoRyADvWvbyI65zyetceI3jwRkGtSwvNrhWP40OPYHE6bGRjC9OlQvbK8qsTjb1469/6VNbyRyxhgQcjFKjI12bcqd4G72I/yazuLlK8sJY5DMMHPpUa7JnwCd68EEfpzzWm8LLUDIcDgEjoaLi5T/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_images(\"fish\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a32409",
   "metadata": {},
   "source": [
    "## Create an App\n",
    "\n",
    "Let's use gradio to create a small app to search through the images.\n",
    "Please fill in the code below:\n",
    "1. Create a [text input](https://www.gradio.app/docs/textbox) where the user can type in a query\n",
    "2. Create a \"Submit\" [button](https://www.gradio.app/docs/button) that finds similar images to the input query and display the resulting images\n",
    "3. A [Gallery component](https://www.gradio.app/docs/gallery) that displays the images\n",
    "\n",
    "\n",
    "**SOLUTION**\n",
    "\n",
    "1. The input is just `gr.Textbox` and you'll want to assign that to some variable\n",
    "2. The submit button is `gr.Button`. Again, assign it to some variable\n",
    "3. The gallery is `gr.Gallery`\n",
    "4. You'll want to define the on click behavior for the button such that it uses\n",
    "   the textbox value as input, calls the `find_images` function you wrote above,\n",
    "   and uses the gallery to display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944b115a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60/1501538491.py:9: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  gallery = gr.Gallery(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        vector_query = gr.Textbox(value=\"fish\", show_label=False)\n",
    "        b1 = gr.Button(\"Submit\")\n",
    "    with gr.Row():\n",
    "        gallery = gr.Gallery(\n",
    "                label=\"Found images\", show_label=False, elem_id=\"gallery\"\n",
    "            ).style(columns=[3], rows=[3], object_fit=\"contain\", height=\"auto\")   \n",
    "        \n",
    "    b1.click(find_images, inputs=vector_query, outputs=gallery)\n",
    "    \n",
    "demo.launch(server_name=\"0.0.0.0\", inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296df52-4941-4857-a86f-e268dd94d048",
   "metadata": {},
   "source": [
    "To view the interface, click on the **Links** button at the bottom of the workspace window.  Then click on **gradio**.  This will open a new browser window with the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97f9ac",
   "metadata": {},
   "source": [
    "Now try a bunch of different queries and see the results.\n",
    "By default CLIP search results leave a lot of room for improvement. More advanced applications in this space can improve these results in a number ways like retraining the model with your own dataset, your own labels, and using image and text vectors to train the index. The details are however beyond the scope of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6354ce",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congrats! \n",
    "\n",
    "Through this exercise, you learned how to use CLIP to generate image and text embeddings. You've mastered how to use vector databases to enable searching through images using natural language. And you even created a simple app to show off your work. \n",
    "\n",
    "Great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb884abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
